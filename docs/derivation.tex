% ============================================================================
% Bayesian Calibration & Inverse Prediction â€” Mathematical Derivation
% ============================================================================
% Compile:  pdflatex derivation.tex   (run twice for references)
% ============================================================================
\documentclass[11pt,a4paper]{article}

% ---- packages --------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{bm}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{fancyhdr}

\geometry{margin=2.5cm}
\hypersetup{colorlinks=true,linkcolor=blue!70!black,citecolor=green!50!black,urlcolor=blue!60!black}

% ---- theorem environments ---------------------------------------------------
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}

% ---- shortcuts --------------------------------------------------------------
\newcommand{\xcal}{\bm{x}^{\mathrm{cal}}}
\newcommand{\ycal}{\bm{y}^{\mathrm{cal}}}
\newcommand{\thetab}{\bm{\theta}}
\newcommand{\phib}{\bm{\phi}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\HN}{\mathcal{N}^{+}}

% ---- header / footer --------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\rhead{\small Bayesian Calibration --- Mathematical Derivation}
\lhead{\small McClelland Lab, UCL}
\cfoot{\thepage}

% =============================================================================
\begin{document}
% =============================================================================

\begin{center}
    {\LARGE\bfseries CaliBR: Calibration with Bayesian inverse Regression}\\[14pt]
    {\large Mathematical Derivation and Assumptions}\\[10pt]
    {\normalsize McClelland Lab, University College London}\\[4pt]
    {\small\today}
\end{center}

\vspace{1em}
\hrule
\vspace{0.5em}
\tableofcontents
\vspace{0.5em}
\hrule
\vspace{1.5em}

% =============================================================================
\section{Introduction}
% =============================================================================

In analytical chemistry, bioassay work and any kind of proxy, where one thing is measured to infer the value of something else, a common workflow is to prepare
standards at known concentrations, measure the instrument response for each,
and fit a calibration curve.  The practical goal, however, is the
\emph{reverse}: given a new instrument reading, estimate the unknown
concentration that produced it.

This is the \textbf{inverse prediction} problem, which is ill-defined in conventional frequentist statistics. A Bayesian treatment resolves this
limitation naturally: we obtain the full joint posterior of the model
parameters, then push every source of uncertainty---parameter estimation
\emph{and} measurement noise---through the inverse to obtain a complete
distribution over the unknown value.  \citet{gelman2004} demonstrate this
approach for immunoassay serial dilution data; the method implemented here
generalises it to arbitrary user-specified calibration functions and variance
models.

This document derives the method implemented in CaliBR.

% =============================================================================
\section{Notation}
% =============================================================================

\begin{center}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{cl}
    \toprule
    Symbol & Meaning \\
    \midrule
    $n$ & Number of calibration points \\
    $x_i$ & Known standard value (e.g.\ concentration) for point $i$ \\
    $y_i$ & Measured instrument response for point $i$ \\
    $\xcal = (x_1,\dots,x_n)^\top$ & Calibration $x$-values \\
    $\ycal = (y_1,\dots,y_n)^\top$ & Calibration $y$-values \\
    $f(x;\,\thetab)$ & User-specified calibration (mean) function \\
    $\thetab = (\theta_1,\dots,\theta_p)^\top$ & Mean model parameters \\
    $\sigma_y$ & Baseline noise standard deviation \\
    $g(\mu,\,\sigma_y;\,\phib)$ & User-specified noise standard deviation function \\
    $\phib$ & Additional variance model parameters (may be empty) \\
    $y^{*}$ & New observed response \\
    $x^{*}$ & Unknown value to be estimated \\
    $\alpha$ & Tail probability for credible intervals ($\alpha = 0.025$ for 95\% CI) \\
    \bottomrule
\end{tabular}
\end{center}

% =============================================================================
\section{The Calibration Model}
\label{sec:model}
% =============================================================================

Two primary assumptions define the generative model: 

\begin{assumption}[Directionality of relationship between variables:] \label{ass:func}
The relationship between the known value $x$ and the instrument response $y$ is
described by a parametric function $f:\R\times\R^p\to\R$, specified by the
user.  For example, $f(x;\,a,b)=a+bx$, or $f(x;\,a,b)=a\,e^{bx}$.  $x$ is independent and its value is known with absolute certainty, while $y$ is dependent on $x$ and is associated with a defined uncertainty. 
\end{assumption}

\begin{assumption}[Structure of unexplained variance:] \label{ass:noise}
Each observation equals the true mean response plus independent {\bf Gaussian noise}.
In the simplest (homoscedastic) case the noise has constant standard
deviation~$\sigma_y$:
\begin{equation}\label{eq:obs-model}
    y_i \;=\; f(x_i;\,\thetab) \;+\; \varepsilon_i,
    \qquad
    \varepsilon_i \;\sim\; \N(0,\,\sigma_y^2),
    \qquad i = 1,\dots,n.
\end{equation}
The noise terms are mutually independent and independent of the standard
values~$x_i$.  An extension to non-constant variance is described in
\Cref{sec:hetero}.
\end{assumption}

\noindent
Under these assumptions the \textbf{likelihood} is
\begin{equation}\label{eq:likelihood}
    p(\ycal \mid \xcal,\,\thetab,\,\sigma_y)
    \;=\; \prod_{i=1}^{n}
        \frac{1}{\sqrt{2\pi}\,\sigma_y}\,
        \exp\!\biggl[-\frac{\bigl(y_i - f(x_i;\,\thetab)\bigr)^{2}}
                           {2\,\sigma_y^{2}}\biggr],
\end{equation}
or equivalently, writing
$\bm{\mu} = \bigl(f(x_1;\thetab),\dots,f(x_n;\thetab)\bigr)^\top$,
\begin{equation}
    \ycal \mid \thetab,\,\sigma_y
    \;\sim\;
    \N\!\bigl(\bm{\mu},\;\sigma_y^{2}\,\mathbf{I}_{n}\bigr).
\end{equation}

% =============================================================================
\section{Heteroscedastic Variance Model}
\label{sec:hetero}
% =============================================================================

In many assays the measurement noise is not constant but varies systematically
with the signal level.  The application allows the user to replace the
constant-variance assumption (\Cref{ass:noise}) with any symbolic variance
equation of the form
\begin{equation}\label{eq:var-generic}
    \sigma_i \;=\; g\!\bigl(\mu_i,\,\sigma_y;\,\phib\bigr),
\end{equation}
where $\mu_i = f(x_i;\,\thetab)$ is the predicted mean at observation~$i$,
$\sigma_y > 0$ is a baseline noise scale, and $\phib$ is a (possibly empty)
vector of additional variance parameters.  The observation model becomes
\begin{equation}\label{eq:obs-hetero}
    y_i \;\sim\; \N\!\bigl(\mu_i,\;\sigma_i^{2}\bigr),
    \qquad
    \sigma_i = g(\mu_i,\,\sigma_y;\,\phib),
    \qquad i = 1,\dots,n,
\end{equation}
and the heteroscedastic likelihood is
\begin{equation}\label{eq:lik-hetero}
    p(\ycal \mid \xcal,\,\thetab,\,\sigma_y,\,\phib)
    \;=\; \prod_{i=1}^{n}
        \frac{1}{\sqrt{2\pi}\,\sigma_i}\,
        \exp\!\biggl[-\frac{(y_i - \mu_i)^{2}}{2\,\sigma_i^{2}}\biggr].
\end{equation}


% =============================================================================
\section{Prior Distributions}
\label{sec:priors}
% =============================================================================

In CaliBR, the user can choose prior distributions for all
parameters through the Advanced Options panel.  The defaults are weakly
informative, independent priors:
\begin{align}
    \theta_j &\sim \N(0,\,10^{2}),
        &&j = 1,\dots,p, \label{eq:prior-theta} \\[4pt]
    \sigma_y &\sim \HN(10),
        \label{eq:prior-sigma}
\end{align}
where $\HN(\tau)$ denotes the half-normal distribution with scale~$\tau$,
i.e.\ a $\N(0,\tau^{2})$ truncated to the positive reals.

\paragraph{Supported prior families.}
For each parameter the user may select from:
Normal, Half-Normal, Uniform, Log-Normal, Exponential, or Gamma distributions,
each with user-specified hyperparameters.

\subsection{Log-Scale Parameterisation}
\label{sec:log-scale}

For parameters that must be positive (e.g.\ rate constants, asymptotes), the
user may opt to model the parameter on the log scale.  For a parameter
$\theta_j > 0$ this means:
\begin{equation}
    \log\theta_j \;\sim\; \pi(\cdot), \qquad
    \theta_j \;=\; \exp(\log\theta_j),
\end{equation}
where $\pi(\cdot)$ is the chosen prior placed on the unconstrained
$\log\theta_j$.  This enforces positivity without requiring bounded priors and
improves sampling geometry for parameters that span several orders of
magnitude.

\paragraph{Rationale.}
The zero-centred normal priors on each~$\theta_j$ are deliberately vague
(standard deviation~10).  The half-normal prior on~$\sigma_y$ enforces
positivity while remaining uninformative over the plausible range of noise
magnitudes.  These defaults perform well across a broad class of calibration
problems.  Users with strong domain knowledge should substitute tighter priors
via the Advanced Options panel.


% =============================================================================
\section{Posterior Distribution}
\label{sec:posterior}
% =============================================================================

Applying Bayes' theorem:
\begin{equation}\label{eq:bayes}
    \boxed{\;
    p(\thetab,\,\sigma_y,\,\phib \mid \ycal,\,\xcal)
    \;=\;
    \frac{p(\ycal \mid \xcal,\,\thetab,\,\sigma_y,\,\phib)\;\;
          p(\thetab,\,\sigma_y,\,\phib)}
         {p(\ycal \mid \xcal)}
    \;}
\end{equation}
where the marginal likelihood (evidence) is
\begin{equation}
    p(\ycal \mid \xcal)
    \;=\;
    \int p(\ycal \mid \xcal,\,\thetab,\,\sigma_y,\,\phib)\;
         p(\thetab,\,\sigma_y,\,\phib)
    \;\mathrm{d}\thetab\;\mathrm{d}\sigma_y\;\mathrm{d}\phib.
\end{equation}
For most nonlinear calibration functions this integral is analytically
intractable, so we approximate the posterior using Markov chain Monte Carlo
sampling.

% =============================================================================
\section{MCMC Sampling via NUTS}
\label{sec:nuts}
% =============================================================================

\subsection{Hamiltonian Monte Carlo}

Hamiltonian Monte Carlo (HMC) \citep{neal2011} augments the parameter space
with auxiliary momentum variables $\bm{r}\in\R^{d}$ and defines the joint
density
\begin{equation}
    p(\thetab,\,\sigma_y,\,\bm{r})
    \;\propto\;
    \exp\!\bigl[\,-U(\thetab,\sigma_y)
                 - \tfrac{1}{2}\,\bm{r}^{\top}\mathbf{M}^{-1}\bm{r}\,\bigr],
\end{equation}
with \emph{potential energy}
\begin{equation}
    U(\thetab,\sigma_y)
    \;=\;
    -\log p(\ycal \mid \xcal,\,\thetab,\,\sigma_y,\,\phib)
    \;-\; \log p(\thetab,\,\sigma_y,\,\phib)
\end{equation}
and mass matrix $\mathbf{M}$ (adapted during warm-up to approximate the
posterior covariance).

HMC simulates Hamiltonian dynamics using the leapfrog integrator with step
size~$\epsilon$:
\begin{align}
    \bm{r}_{t+\epsilon/2}
        &= \bm{r}_{t}
           - \tfrac{\epsilon}{2}\,\nabla_{\thetab}\,U(\thetab_{t}),
    \\
    \thetab_{t+\epsilon}
        &= \thetab_{t}
           + \epsilon\,\mathbf{M}^{-1}\,\bm{r}_{t+\epsilon/2},
    \\
    \bm{r}_{t+\epsilon}
        &= \bm{r}_{t+\epsilon/2}
           - \tfrac{\epsilon}{2}\,\nabla_{\thetab}\,U(\thetab_{t+\epsilon}).
\end{align}
After $L$ leapfrog steps the proposal is accepted with probability
$\min\!\bigl(1,\,\exp(-\Delta H)\bigr)$, where $\Delta H$ is the change in
the Hamiltonian.

\subsection{The No-U-Turn Sampler}

NUTS \citep{hoffman2014} removes the need to choose~$L$ by building a balanced
binary tree of leapfrog steps.  The tree doubles in size until a ``U-turn''
is detected:
\begin{equation}
    \bm{r}\cdot(\thetab^{+}-\thetab^{-}) < 0
    \qquad\text{or}\qquad
    \bm{r}\cdot(\thetab^{-}-\thetab^{+}) < 0,
\end{equation}
where $\thetab^{+}$ and $\thetab^{-}$ are the trajectory endpoints.
A multinomial scheme selects the next state from the trajectory, weighted by
the unnormalised density.  The step size~$\epsilon$ is tuned during warm-up
via dual averaging \citep{nesterov2009} to target an acceptance rate of
${\sim}\,0.8$.

\subsection{Warm-up, Sampling, and Diagnostics}

\begin{enumerate}[nosep]
    \item \textbf{Warm-up.}  The step size and mass matrix are adapted; these
          draws are discarded.
    \item \textbf{Sampling.}  $S$~draws are collected from each of
          $C$~independent chains, giving $N = S \times C$ posterior samples
          $\{(\thetab^{(s)},\,\sigma_y^{(s)},\,\phib^{(s)})\}_{s=1}^{N}$.
    \item \textbf{Convergence checks.}  The application reports:
    \begin{itemize}[nosep]
        \item $\hat{R}$ (Gelman--Rubin statistic): values $\lesssim 1.01$
              indicate convergence \citep{gelman2013}.
        \item Effective sample size ($n_{\mathrm{eff}}$): the number of
              effectively independent draws after accounting for
              autocorrelation.
        \item Monte Carlo standard error (MCSE): the precision of the
              posterior mean estimate.
    \end{itemize}
\end{enumerate}

% =============================================================================
\section{Inverse Prediction}
\label{sec:inverse}
% =============================================================================

\subsection{Problem Statement}

Given a new instrument reading $y^{*}$, we want the posterior predictive
distribution of the unknown value $x^{*}$ that produced it.  

\begin{assumption}[Consistency in generative process between calibration and new observation] 
We assume that the new observation arises from the same process as the calibration data:
\begin{equation}\label{eq:new-obs}
    y^{*} \;=\; f(x^{*};\,\thetab) \;+\; \varepsilon^{*},
    \qquad
    \varepsilon^{*} \;\sim\; \N\!\bigl(0,\,\sigma^{*2}\bigr),
\end{equation}
where the noise standard deviation is
\begin{equation}
    \sigma^{*} \;=\; g\!\bigl(\mu^{*},\,\sigma_y;\,\phib\bigr),
    \qquad
    \mu^{*} = f(x^{*};\,\thetab).
\end{equation}
Because $x^{*}$ is unknown, $\mu^{*}$ is also unknown.  
\end{assumption}

We approximate the variance at $x^{*}$ by substituting the observed value~$y^{*}$ as a plug-in for~$\mu^{*}$:
\begin{equation}\label{eq:sigma-star-approx}
    \sigma^{*} \;\approx\; g\!\bigl(y^{*},\,\sigma_y;\,\phib\bigr).
\end{equation}
This approximation is accurate when the signal-to-noise ratio is moderate to
high (i.e.\ $y^{*} \approx \mu^{*}$).  In the constant-variance case
$g = \sigma_y$ and no approximation is needed.

\subsection{Posterior Predictive Distribution of \texorpdfstring{$x^{*}$}{x*}}

The target distribution is obtained by marginalising over the posterior:
\begin{equation}\label{eq:ppd}
    \boxed{\;
    p(x^{*} \mid y^{*},\,\ycal,\,\xcal)
    \;=\;
    \int p(x^{*} \mid y^{*},\,\thetab,\,\sigma_y,\,\phib)\;\;
         p(\thetab,\,\sigma_y,\,\phib \mid \ycal,\,\xcal)
    \;\mathrm{d}\thetab\;\mathrm{d}\sigma_y\;\mathrm{d}\phib
    \;}
\end{equation}
This integral propagates \emph{both} parameter uncertainty and measurement
noise into the prediction.  We evaluate it by Monte Carlo:

For each posterior draw
$(\thetab^{(s)},\,\sigma_y^{(s)},\,\phib^{(s)})$, $s=1,\dots,N$:
\begin{enumerate}[nosep]
    \item \textbf{Compute noise scale:}\;
          $\sigma^{*(s)} = g\!\bigl(y^{*},\,\sigma_y^{(s)};\,\phib^{(s)}\bigr)$
          \;\;(using the plug-in approximation~\eqref{eq:sigma-star-approx}).
    \item \textbf{Add noise:}\;
          $\tilde{y}^{(s)} = y^{*} + \epsilon^{(s)}$\;
          where\; $\epsilon^{(s)} \sim \N(0,\,\sigma^{*(s)2})$.
    \item \textbf{Invert:}\;
          $x^{*(s)} = f^{-1}\!\bigl(\tilde{y}^{(s)};\,\thetab^{(s)}\bigr)$.
\end{enumerate}
The resulting collection $\{x^{*(s)}\}_{s=1}^{N}$ is a sample from
$p(x^{*} \mid y^{*},\,\ycal,\,\xcal)$.


\subsection{Inversion Methods}

\paragraph{Symbolic inverse.}
When $f$ is algebraically invertible, SymPy computes
$x = f^{-1}(y;\,\thetab)$ in closed form.  If multiple real solutions exist
(e.g.\ for a quadratic), the one closest to the centroid of the calibration
$x$-values is selected.

\paragraph{Numerical inverse.}
When no closed form exists, Brent's root-finding method
\citep{brent1973} solves
\begin{equation}
    f(x;\,\thetab^{(s)}) - \tilde{y}^{(s)} = 0
\end{equation}
over the interval
$\bigl[x_{\min} - 3\,\Delta x,\;\; x_{\max} + 3\,\Delta x\bigr]$
where $\Delta x = x_{\max} - x_{\min}$.
Brent's method combines bisection, secant, and inverse quadratic
interpolation, guaranteeing convergence whenever a sign change exists in the
search interval.

\subsection{Credible Intervals and Point Estimates}

From the $N$ draws $\{x^{*(s)}\}$ (after discarding any non-finite values from
failed inversions), the $100(1-2\alpha)\%$ equal-tailed credible interval is
\begin{equation}
    \mathrm{CI}_{1-2\alpha}
    \;=\;
    \bigl[\,Q_{\alpha},\;\; Q_{1-\alpha}\,\bigr],
\end{equation}
where $Q_{q}$ denotes the $q$-th sample quantile and $\alpha$ is the tail
probability (e.g.\ $\alpha = 0.025$ for a 95\% interval).  The application
also reports:
\begin{align}
    \hat{x}_{\mathrm{median}} &= Q_{0.5}, \\[2pt]
    \hat{x}_{\mathrm{mean}}
        &= \frac{1}{N}\sum_{s=1}^{N} x^{*(s)}, \\[2pt]
    \hat{\sigma}_{x}
        &= \sqrt{\frac{1}{N-1}
           \sum_{s=1}^{N}\bigl(x^{*(s)} - \hat{x}_{\mathrm{mean}}\bigr)^{2}}.
\end{align}

% =============================================================================
\section{Residual Diagnostics}
\label{sec:diagnostics}
% =============================================================================

After fitting the model, the CaliBR runs automated diagnostics to help the
user assess whether the chosen equation and the noise assumptions are adequate, before giving the option to proceed to inverting to find $x^{*}$. 

\subsection{Residuals}

Residuals are evaluated at the posterior median parameters
$\hat{\thetab} = \mathrm{median}\{\thetab^{(s)}\}$ and posterior median
variance parameters $\hat{\phib}$:
\begin{equation}
    e_i \;=\; y_i \;-\; f(x_i;\,\hat{\thetab}),
    \qquad i = 1,\dots,n.
\end{equation}
Standardised residuals are obtained by dividing by the model-predicted noise
standard deviation at each point:
\begin{equation}
    r_i \;=\; \frac{e_i}{\hat{\sigma}_i},
    \qquad
    \hat{\sigma}_i = g(\hat{\mu}_i,\,\hat{\sigma}_y;\,\hat{\phib}),
    \qquad
    \hat{\mu}_i = f(x_i;\,\hat{\thetab}).
\end{equation}
Two plots are shown: raw residuals $e_i$ versus~$x_i$ (with posterior
prediction intervals) and standardised residuals $r_i$ versus~$x_i$.  If the
variance model is adequate, the standardised residuals should scatter randomly
around zero with roughly constant spread.

\subsection{Breusch--Pagan Test}

The Breusch--Pagan test \citep{breusch1979} checks whether the
\emph{standardised} residuals~$r_i$ still exhibit variance that depends
on~$x_i$, i.e.\ whether the chosen variance model has successfully removed
the heteroscedasticity:
\begin{equation}
    H_0:\;\Var(r_i) = \mathrm{const}\;\;\forall\,i
    \qquad\text{vs.}\qquad
    H_1:\;\Var(r_i) = h(\gamma_0 + \gamma_1\,x_i).
\end{equation}
The procedure regresses $r_i^{2}$ on a constant and $x_i$ by OLS.  The LM
statistic is
\begin{equation}
    \mathrm{LM} \;=\; n\,R^{2},
\end{equation}
where $R^{2}$ is the coefficient of determination of the auxiliary regression.
Under $H_0$, $\mathrm{LM} \sim \chi^{2}_{1}$ asymptotically.  A $p$-value
below 0.05 indicates that residual spread still varies with~$x_i$; this
suggests a more flexible variance model may be needed, or that a
variance-stabilising transformation (e.g.\ $\log y$ or $\sqrt{y}$) should be
applied before fitting.

\subsection{Practical Guidance}

Based on the diagnostics the application presents the following advice:
\begin{itemize}[nosep]
    \item \textbf{Curved or trending residuals:} The chosen functional form does not adequately capture the
          calibration relationship, so a different model is required (even if it passes the heteroscedasticity test). This step is best addressed by revisiting the physical principles underlying the mean model equation. Employing purely empirical fits (e.g. arbitrarily adding additional polynomial terms) is not good practice.  
    \item \textbf{Fan-shaped spread in raw residuals:} enable or revise the
          \textbf{variance model} to let the model learn how noise scales with
          the mean response.  Alternatively, apply a variance-stabilising
          transformation such as $\log(y)$ or $\sqrt{y}$ before fitting.
    \item \textbf{Significant Breusch--Pagan test ($p < 0.05$) on standardised
          residuals:} the current variance model does not fully capture the
          heteroscedasticity; consider a different variance equation.  Note:  The 0.05 significance level is arbitrary, so treat with care. 
\end{itemize}

% =============================================================================
\section{Summary of Assumptions}
\label{sec:assumptions}
% =============================================================================

\begin{enumerate}[nosep,leftmargin=2em]
    \item The user-specified function $f(x;\thetab)$ adequately describes the
          calibration relationship
          (\Cref{ass:func}).
    \item Observation noise is additive and Gaussian, with standard deviation
          given by the user-specified function
          $\sigma_i = g(\mu_i,\,\sigma_y;\,\phib)$
          (\Cref{ass:noise}, \Cref{sec:hetero}).
    \item The log-posterior is differentiable with respect to all continuous
          parameters
          (\Cref{ass:diff}).
    \item Prior distributions are weakly informative and mutually independent
          (user-configurable via Advanced Options;
          \Cref{sec:priors}).
    \item New measurements follow the same generative process as the
          calibration data (no distribution shift).
\end{enumerate}

\paragraph{When assumptions may be violated.}
\begin{itemize}[nosep]
    \item \emph{Wrong functional form.}  Curved or systematic residuals
          suggest the model does not capture the true relationship; try a
          different equation.
    \item \emph{Inadequate variance model.}  The Breusch--Pagan test will flag
          remaining heteroscedasticity in the standardised residuals; revise
          the variance equation or apply a variance-stabilising transform
          (e.g.\ $\log y$) and re-fit.
    \item \emph{Non-Gaussian noise.}  CaliBR currently assumes Gaussian noise. for most applications this should be fine (future implementations will incorporate alternate noise distributions)
        \item \emph{Correlated observations.}  Strictly this approach is not designed to estimate uncertainty for measurements that are
          time-dependent or spatially structured (future implementations will address this).
\end{itemize}

% =============================================================================
\section{Implementation Notes}
\label{sec:software}
% =============================================================================

\begin{itemize}[nosep]
    \item \textbf{Equation parsing:} SymPy with implicit multiplication and
          \texttt{convert\_xor} transformations.  Both the mean function
          $f(x;\thetab)$ and the variance function $g(\mu,\sigma_y;\phib)$
          are parsed symbolically, enabling automatic inversion and
          differentiation.
    \item \textbf{Automatic differentiation:} PyTensor provides exact
          gradients for the NUTS sampler.
    \item \textbf{Sampling:} PyMC\,$\ge$\,5.10 with the default NUTS
          implementation.
    \item \textbf{Diagnostics:} ArviZ for convergence statistics; statsmodels
          for the Breusch--Pagan test.
    \item \textbf{Numerical inversion:} SciPy's \texttt{brentq} with
          tolerance~$10^{-10}$.
\end{itemize}


\section{Built-in Functions and Examples}
... to come. 



% =============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Breusch and Pagan(1979)]{breusch1979}
T.~S. Breusch and A.~R. Pagan.
\newblock A simple test for heteroscedasticity and random coefficient variation.
\newblock \emph{Econometrica}, 47(5):1287--1294, 1979.

\bibitem[Brent(1973)]{brent1973}
R.~P. Brent.
\newblock \emph{Algorithms for Minimization without Derivatives}.
\newblock Prentice-Hall, 1973.

\bibitem[Gelman et~al.(2004)]{gelman2004}
A.~Gelman, G.~L. Chew, and M.~Shnaidman.
\newblock Bayesian analysis of serial dilution assays.
\newblock \emph{Biometrics}, 60(2):407--417, 2004.

\bibitem[Gelman et~al.(2013)]{gelman2013}
A.~Gelman, J.~B. Carlin, H.~S. Stern, D.~B. Dunson, A.~Vehtari, and
D.~B. Rubin.
\newblock \emph{Bayesian Data Analysis}.
\newblock Chapman \& Hall/CRC, 3rd edition, 2013.

\bibitem[Hoffman and Gelman(2014)]{hoffman2014}
M.~D. Hoffman and A.~Gelman.
\newblock The {No-U-Turn} sampler: adaptively setting path lengths in
{Hamiltonian Monte Carlo}.
\newblock \emph{Journal of Machine Learning Research}, 15:1593--1623, 2014.

\bibitem[Neal(2011)]{neal2011}
R.~M. Neal.
\newblock {MCMC} using {Hamiltonian} dynamics.
\newblock In S.~Brooks, A.~Gelman, G.~L. Jones, and X.-L. Meng, editors,
\emph{Handbook of Markov Chain Monte Carlo}, chapter~5.
Chapman \& Hall/CRC, 2011.

\bibitem[Nesterov(2009)]{nesterov2009}
Y.~Nesterov.
\newblock Primal-dual subgradient methods for convex problems.
\newblock \emph{Mathematical Programming}, 120(1):221--259, 2009.

\bibitem[Salvatier et~al.(2016)]{salvatier2016}
J.~Salvatier, T.~V. Wiecki, and C.~Fonnesbeck.
\newblock Probabilistic programming in {Python} using {PyMC3}.
\newblock \emph{PeerJ Computer Science}, 2:e55, 2016.

\end{thebibliography}

% =============================================================================
\end{document}
% =============================================================================
