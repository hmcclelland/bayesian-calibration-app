% ============================================================================
% Bayesian Calibration & Inverse Prediction â€” Mathematical Derivation
% ============================================================================
% Compile:  pdflatex derivation.tex   (run twice for references)
% ============================================================================
\documentclass[11pt,a4paper]{article}

% ---- packages --------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{bm}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}

\geometry{margin=2.5cm}
\hypersetup{colorlinks=true,linkcolor=blue!70!black,citecolor=green!50!black,urlcolor=blue!60!black}

% ---- theorem environments ---------------------------------------------------
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}

% ---- shortcuts --------------------------------------------------------------
\newcommand{\yobs}{\bm{y}^{\mathrm{obs}}}
\newcommand{\xcal}{\bm{x}^{\mathrm{cal}}}
\newcommand{\ycal}{\bm{y}^{\mathrm{cal}}}
\newcommand{\thetab}{\bm{\theta}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\HN}{\mathcal{N}^{+}}
\DeclareMathOperator*{\argmin}{arg\,min}

% ---- header / footer --------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\rhead{\small Bayesian Calibration --- Mathematical Derivation}
\lhead{\small McClelland Lab, UCL}
\cfoot{\thepage}

% =============================================================================
\begin{document}
% =============================================================================

\begin{center}
    {\LARGE\bfseries Bayesian Calibration \&\\[4pt] Inverse Prediction}\\[12pt]
    {\large Mathematical Derivation and Assumptions}\\[8pt]
    {\normalsize McClelland Lab, University College London}\\[4pt]
    {\small\today}
\end{center}

\vspace{1em}
\hrule
\vspace{0.5em}
\tableofcontents
\vspace{0.5em}
\hrule
\vspace{1.5em}

% =============================================================================
\section{Introduction and Motivation}
% =============================================================================

In analytical chemistry and bioassay work the standard calibration workflow is:
\begin{enumerate}[nosep]
    \item Prepare standards at known concentrations $x_1,\dots,x_n$.
    \item Measure the instrument response $y_i$ for each standard.
    \item Fit a calibration curve $y = f(x;\,\thetab)$.
    \item For a new measurement $y^{*}$, \emph{invert} the curve to estimate the
          unknown concentration $x^{*}$.
\end{enumerate}

Step~4 is the \textbf{inverse prediction} (or \emph{calibration}) problem.
Classical frequentist approaches (e.g.\ Fieller's theorem, Wald intervals)
provide approximate confidence intervals for $x^{*}$, but they rely on
asymptotic normality, do not propagate the full parameter uncertainty, and
become unreliable for nonlinear models.

The Bayesian framework solves this naturally: we obtain the full joint
posterior distribution of the model parameters, then propagate every source of
uncertainty---parameter uncertainty \emph{and} measurement noise---through the
inverse function to produce a posterior predictive distribution for $x^{*}$.

This document provides a self-contained mathematical derivation of the method
implemented in the accompanying Streamlit application.

% =============================================================================
\section{Notation}
% =============================================================================

\begin{center}
\begin{tabular}{cl}
    \toprule
    Symbol & Meaning \\
    \midrule
    $n$ & Number of calibration observations \\
    $x_i\in\R$ & Known standard value (concentration) for observation $i$ \\
    $y_i\in\R$ & Measured instrument response for observation $i$ \\
    $\xcal=(x_1,\dots,x_n)^\top$ & Vector of calibration $x$-values \\
    $\ycal=(y_1,\dots,y_n)^\top$ & Vector of calibration $y$-values \\
    $f(x;\,\thetab)$ & Forward (calibration) function \\
    $\thetab=(\theta_1,\dots,\theta_p)^\top$ & Model parameters \\
    $\sigma$ & Observation noise standard deviation (homoscedastic model) \\
    $\sigma_i$ & Observation-level noise s.d.\ (heteroscedastic model) \\
    $y^{*}$ & A new observed response for which we wish to find $x^{*}$ \\
    $x^{*}$ & The unknown quantity to be estimated via inverse prediction \\
    \bottomrule
\end{tabular}
\end{center}

% =============================================================================
\section{Forward Model}
\label{sec:forward}
% =============================================================================

\subsection{Homoscedastic Gaussian Likelihood}

\begin{assumption}[Functional form]\label{ass:functional}
    The calibration relationship is described by a known function
    $f:\R\times\R^p\to\R$ that is continuous and differentiable almost
    everywhere with respect to both $x$ and $\thetab$.  The user specifies this
    function (e.g.\ $f(x;\,a,b)=a+bx$, or $f(x;\,a,b)=a\,e^{bx}$).
\end{assumption}

\begin{assumption}[Additive Gaussian noise]\label{ass:noise}
    Observations are generated by
    \begin{equation}\label{eq:likelihood-scalar}
        y_i = f(x_i;\,\thetab) + \varepsilon_i,
        \qquad \varepsilon_i \stackrel{\mathrm{iid}}{\sim} \N(0,\,\sigma^2),
        \qquad i=1,\dots,n.
    \end{equation}
\end{assumption}

\begin{assumption}[Independence]\label{ass:indep}
    The noise terms $\varepsilon_1,\dots,\varepsilon_n$ are mutually independent
    and independent of the true concentrations $x_1,\dots,x_n$.
\end{assumption}

Under these assumptions the likelihood function is
\begin{equation}\label{eq:likelihood}
    p(\ycal \mid \xcal,\,\thetab,\,\sigma)
    = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\,\sigma}
      \exp\!\Bigl[-\frac{(y_i - f(x_i;\,\thetab))^2}{2\sigma^2}\Bigr].
\end{equation}

Equivalently, in vector form with $\bm{\mu}=(f(x_1;\thetab),\dots,f(x_n;\thetab))^\top$:
\begin{equation}
    \ycal \mid \thetab,\sigma \;\sim\; \N(\bm{\mu},\;\sigma^2 \mathbf{I}_n).
\end{equation}

% =============================================================================
\subsection{Heteroscedastic Gaussian Likelihood}
\label{sec:hetero}

\Cref{ass:noise} assumes constant variance.  In many assays the noise variance
grows with the signal level.  We relax this with:

\begin{assumption}[Non-constant variance]\label{ass:hetero}
    The observation noise has standard deviation that depends on the predicted
    mean $\mu_i = f(x_i;\,\thetab)$:
    \begin{equation}\label{eq:hetero-obs}
        y_i = f(x_i;\,\thetab) + \varepsilon_i,
        \qquad \varepsilon_i \sim \N(0,\,\sigma_i^2),
    \end{equation}
    where $\sigma_i = g(\mu_i;\,\bm{\phi})$ for some variance function $g$
    with its own parameters $\bm{\phi}$.
\end{assumption}

The application offers two variance functions:

\paragraph{Linear variance model.}
\begin{equation}\label{eq:var-linear}
    \sigma_i = \sigma_0 + \sigma_1\,|\mu_i|,
    \qquad \sigma_0>0,\;\sigma_1\ge0.
\end{equation}
Here $\sigma_0$ captures a baseline noise floor and $\sigma_1$ captures the
proportional increase in noise with signal magnitude.

\paragraph{Power-of-the-mean variance model.}
\begin{equation}\label{eq:var-power}
    \sigma_i = \sigma_0\,|\mu_i|^{\delta},
    \qquad \sigma_0>0,\;\delta\ge0.
\end{equation}
When $\delta=0$ this reduces to constant variance; when $\delta=1$ it gives
a coefficient-of-variation model (constant relative standard deviation).

The heteroscedastic likelihood is
\begin{equation}\label{eq:likelihood-hetero}
    p(\ycal \mid \xcal,\,\thetab,\,\bm{\phi})
    = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\,\sigma_i}
      \exp\!\Bigl[-\frac{(y_i - \mu_i)^2}{2\sigma_i^2}\Bigr],
\end{equation}
where each $\sigma_i$ is given by \eqref{eq:var-linear} or
\eqref{eq:var-power}.

% =============================================================================
\section{Prior Distributions}
\label{sec:priors}
% =============================================================================

\begin{assumption}[Weakly informative priors]\label{ass:priors}
    We adopt the following independent prior distributions.
\end{assumption}

\subsection{Homoscedastic Model}

\begin{align}
    \theta_j &\sim \N(0,\,10^2), \qquad j=1,\dots,p, \label{eq:prior-theta}\\
    \sigma   &\sim \HN(0,\,10^2), \label{eq:prior-sigma}
\end{align}
where $\HN(0,\,\tau^2)$ denotes the half-normal distribution (i.e.\ a normal
distribution truncated to $(0,\infty)$).

\subsection{Heteroscedastic Model}

For the variance-function parameters:
\begin{align}
    \theta_j &\sim \N(0,\,10^2), \qquad j=1,\dots,p, \\
    \sigma_0 &\sim \HN(0,\,10^2), \label{eq:prior-sigma0}\\
    \sigma_1 &\sim \HN(0,\,1^2), \qquad\text{(linear model)}\label{eq:prior-sigma1}\\
    \delta   &\sim \HN(0,\,1^2). \qquad\text{(power model)}\label{eq:prior-delta}
\end{align}

\paragraph{Rationale.}
The normal priors on $\thetab$ are centred at zero with standard deviation~10,
which is deliberately vague for standardised data.  The half-normal priors on
scale parameters ensure positivity while remaining weakly informative.  These
defaults work well across a wide range of calibration problems; users with
strong domain knowledge may wish to substitute more informative priors in the
source code.

\paragraph{Prior independence.}
All parameters are assumed \emph{a priori} independent:
\begin{equation}
    p(\thetab,\sigma) = \Bigl[\prod_{j=1}^{p}p(\theta_j)\Bigr]\,p(\sigma).
\end{equation}

% =============================================================================
\section{Posterior Distribution}
\label{sec:posterior}
% =============================================================================

By Bayes' theorem the joint posterior is
\begin{equation}\label{eq:bayes}
    \boxed{
    p(\thetab,\sigma \mid \ycal,\xcal)
    = \frac{p(\ycal \mid \xcal,\thetab,\sigma)\;p(\thetab,\sigma)}
           {p(\ycal \mid \xcal)},
    }
\end{equation}
where the marginal likelihood (evidence) is
\begin{equation}
    p(\ycal\mid\xcal)
    = \int p(\ycal\mid\xcal,\thetab,\sigma)\,p(\thetab,\sigma)
      \;\mathrm{d}\thetab\;\mathrm{d}\sigma.
\end{equation}

For most nonlinear calibration functions $f$ this integral is analytically
intractable, so we resort to Markov chain Monte Carlo (MCMC) sampling.

% =============================================================================
\section{MCMC Sampling via NUTS}
\label{sec:nuts}
% =============================================================================

\subsection{Hamiltonian Monte Carlo}

Hamiltonian Monte Carlo (HMC) augments the parameter space with auxiliary
momentum variables $\bm{r}\in\R^d$ (where $d=p+1$ for the homoscedastic model)
and defines a joint density proportional to
\begin{equation}
    p(\thetab,\sigma,\bm{r})
    \propto \exp\!\bigl[-U(\thetab,\sigma) - \tfrac{1}{2}\bm{r}^\top\mathbf{M}^{-1}\bm{r}\bigr],
\end{equation}
where the \emph{potential energy} is
\begin{equation}
    U(\thetab,\sigma)
    = -\log p(\ycal\mid\xcal,\thetab,\sigma) - \log p(\thetab,\sigma),
\end{equation}
and $\mathbf{M}$ is a mass matrix (typically adapted during warm-up to
approximate the posterior covariance).

HMC simulates Hamiltonian dynamics via the leapfrog integrator:
\begin{align}
    \bm{r}_{t+\epsilon/2} &= \bm{r}_t - \frac{\epsilon}{2}\,\nabla_{\thetab}U(\thetab_t), \\
    \thetab_{t+\epsilon}   &= \thetab_t + \epsilon\,\mathbf{M}^{-1}\bm{r}_{t+\epsilon/2}, \\
    \bm{r}_{t+\epsilon}   &= \bm{r}_{t+\epsilon/2} - \frac{\epsilon}{2}\,\nabla_{\thetab}U(\thetab_{t+\epsilon}),
\end{align}
for $L$ leapfrog steps with step size $\epsilon$.  The proposal is accepted
with probability $\min(1,\,\exp(-\Delta H))$ where $\Delta H$ is the change in
Hamiltonian.

\begin{assumption}[Differentiability for HMC]\label{ass:diff}
    The log-posterior $\log p(\thetab,\sigma\mid\ycal,\xcal)$ is differentiable
    with respect to all continuous parameters.  This is required for the
    gradient-based HMC sampler.  SymPy-parsed expressions are analytically
    differentiable, and PyTensor provides automatic differentiation.
\end{assumption}

\subsection{The No-U-Turn Sampler (NUTS)}

NUTS \citep{hoffman2014} eliminates the need to hand-tune $L$ and $\epsilon$.
It builds a balanced binary tree of leapfrog steps, doubling the trajectory
length until a ``U-turn'' criterion is met:
\begin{equation}
    \bm{r}\cdot(\thetab^{+} - \thetab^{-}) < 0
    \quad\text{or}\quad
    \bm{r}\cdot(\thetab^{-} - \thetab^{+}) < 0,
\end{equation}
where $\thetab^{+}$ and $\thetab^{-}$ are the forward and backward ends of the
trajectory.  A multinomial sampling scheme selects the next state from the
trajectory, weighted by the un-normalised density.

The step size $\epsilon$ is adapted during the warm-up phase using dual
averaging \citep{nesterov2009} to target an acceptance probability of
approximately 0.8.

\subsection{Warm-up and Adaptation}

During the warm-up (tuning) phase:
\begin{enumerate}[nosep]
    \item The step size $\epsilon$ is adapted.
    \item The mass matrix $\mathbf{M}$ is estimated from sample covariance.
    \item Warm-up draws are \emph{discarded} and not used for inference.
\end{enumerate}

After warm-up, $S$ draws $\{(\thetab^{(s)},\sigma^{(s)})\}_{s=1}^{S}$ are
collected from each of $C$ independent chains, giving a total of $N=S\times C$
posterior draws.

\subsection{Convergence Diagnostics}

The application reports the ArviZ summary statistics including:
\begin{itemize}[nosep]
    \item $\hat{R}$ (Gelman--Rubin statistic): values $\lesssim 1.01$ indicate
          convergence.
    \item Effective sample size ($n_{\mathrm{eff}}$): the number of effectively
          independent draws, accounting for autocorrelation.
    \item Monte Carlo standard error (MCSE): the standard error of the
          posterior mean estimate.
\end{itemize}

% =============================================================================
\section{Inverse Prediction}
\label{sec:inverse}
% =============================================================================

\subsection{Problem Statement}

Given a new observed response $y^{*}$, we seek the posterior predictive
distribution of the unknown $x^{*}$ that produced it.

\begin{assumption}[New observation model]\label{ass:new-obs}
    The new measurement follows the same generative process as the calibration
    data:
    \begin{equation}\label{eq:new-obs}
        y^{*} = f(x^{*};\,\thetab) + \varepsilon^{*},
        \qquad \varepsilon^{*}\sim\N(0,\,\sigma^{*2}),
    \end{equation}
    where $\sigma^{*}=\sigma$ in the homoscedastic case, or
    $\sigma^{*}=g(f(x^{*};\thetab);\,\bm{\phi})$ in the heteroscedastic case.
\end{assumption}

\subsection{Posterior Predictive Distribution of \texorpdfstring{$x^{*}$}{x*}}

The key quantity is
\begin{equation}\label{eq:ppd}
    \boxed{
    p(x^{*} \mid y^{*},\,\ycal,\,\xcal)
    = \int p(x^{*} \mid y^{*},\,\thetab,\,\sigma)\;
           p(\thetab,\sigma\mid\ycal,\xcal)
      \;\mathrm{d}\thetab\;\mathrm{d}\sigma.
    }
\end{equation}

This integral marginalises over the full posterior uncertainty in $\thetab$ and
$\sigma$.  We approximate it by Monte Carlo:

\begin{proposition}[Monte Carlo inverse prediction]\label{prop:mc-inverse}
    For each posterior draw $(\thetab^{(s)},\sigma^{(s)})$, $s=1,\dots,N$:
    \begin{enumerate}[nosep]
        \item Sample a ``noisy'' response:
              $\tilde{y}^{(s)} = y^{*} + \epsilon^{(s)}$
              where $\epsilon^{(s)}\sim\N(0,\,(\sigma^{*(s)})^2)$.

              In the homoscedastic case $\sigma^{*(s)}=\sigma^{(s)}$.

              In the heteroscedastic case we approximate
              $\sigma^{*(s)} \approx g(y^{*};\,\bm{\phi}^{(s)})$, using
              $y^{*}$ as a proxy for $|f(x^{*};\thetab^{(s)})|$.

        \item Invert the forward model:
              $x^{*(s)} = f^{-1}(\tilde{y}^{(s)};\,\thetab^{(s)})$.
    \end{enumerate}
    The collection $\{x^{*(s)}\}_{s=1}^{N}$ is a sample from the posterior
    predictive distribution $p(x^{*}\mid y^{*},\ycal,\xcal)$.
\end{proposition}

\begin{proof}
By the law of total probability:
\begin{align}
    p(x^{*}\mid y^{*},\ycal,\xcal)
    &= \int p(x^{*}\mid y^{*},\thetab,\sigma)\,
            p(\thetab,\sigma\mid\ycal,\xcal)
       \;\mathrm{d}\thetab\,\mathrm{d}\sigma.
\end{align}
For a given $(\thetab,\sigma)$, the forward model is deterministic, so
$x^{*}=f^{-1}(y^{*}-\varepsilon^{*};\,\thetab)$ where
$\varepsilon^{*}\sim\N(0,\sigma^2)$.  Sampling
$\tilde{y}^{(s)}=y^{*}+\epsilon^{(s)}$ with $\epsilon^{(s)}\sim\N(0,\sigma^{(s)2})$
is equivalent to sampling the ``true'' response $y^{*}-\varepsilon^{*}$ from
the predictive distribution at that draw.  Since $(\thetab^{(s)},\sigma^{(s)})$
are draws from $p(\thetab,\sigma\mid\ycal,\xcal)$, the composition yields draws
from the marginal \eqref{eq:ppd}.
\end{proof}

\subsection{Inversion Methods}

\paragraph{Symbolic inverse.}
When $f$ is algebraically invertible with respect to $x$, SymPy computes the
closed-form solution $x=f^{-1}(y;\,\thetab)$.  If multiple real solutions exist
(e.g.\ for a quadratic), the solution closest to the centroid of the
calibration $x$-values is selected.

\paragraph{Numerical inverse.}
When no closed-form inverse exists, Brent's method is applied to find the root
of
\begin{equation}
    h(x) = f(x;\,\thetab^{(s)}) - \tilde{y}^{(s)} = 0
\end{equation}
over a search interval $[x_{\min}-3\Delta x,\;x_{\max}+3\Delta x]$ where
$\Delta x = x_{\max}-x_{\min}$.  Brent's method combines bisection,
secant, and inverse quadratic interpolation, guaranteeing convergence for
continuous functions with a sign change.

\subsection{Credible Intervals}

From the $N$ draws $\{x^{*(s)}\}$, after removing any non-finite values
(from failed inversions), the $100(1-\alpha)\%$ equal-tailed credible interval
is
\begin{equation}
    \mathrm{CI}_{1-\alpha}
    = \bigl[Q_{\alpha/2},\; Q_{1-\alpha/2}\bigr],
\end{equation}
where $Q_q$ denotes the $q$-th quantile of the empirical distribution.

Point estimates reported:
\begin{align}
    \hat{x}_{\mathrm{median}} &= Q_{0.5}, \\
    \hat{x}_{\mathrm{mean}}   &= \frac{1}{N}\sum_{s=1}^{N}x^{*(s)}, \\
    \hat{\sigma}_{x}          &= \sqrt{\frac{1}{N-1}\sum_{s=1}^{N}(x^{*(s)}-\hat{x}_{\mathrm{mean}})^2}.
\end{align}

% =============================================================================
\section{Residual Diagnostics}
\label{sec:diagnostics}
% =============================================================================

After fitting the homoscedastic model, the application performs residual
diagnostics to assess model adequacy.

\subsection{Residual Computation}

The residuals are computed at the posterior median parameter values
$\hat{\thetab}=\mathrm{median}\{\thetab^{(s)}\}$:
\begin{equation}
    e_i = y_i - f(x_i;\,\hat{\thetab}), \qquad i=1,\dots,n.
\end{equation}

\subsection{Breusch--Pagan Test for Heteroscedasticity}

The Breusch--Pagan test \citep{breusch1979} tests
\begin{equation}
    H_0:\;\Var(\varepsilon_i)=\sigma^2\;\;\forall\,i
    \qquad\text{vs.}\qquad
    H_1:\;\Var(\varepsilon_i)=h(\bm{z}_i^\top\bm{\gamma}),
\end{equation}
where $\bm{z}_i$ is a vector of regressors (here, the fitted values
$\hat{y}_i$).

The test procedure:
\begin{enumerate}[nosep]
    \item Compute squared residuals $e_i^2$.
    \item Regress $e_i^2$ on $\bm{z}_i$ by OLS.
    \item The LM statistic is $\mathrm{LM}=\frac{1}{2}\,\mathrm{ESS}$, where
          ESS is the explained sum of squares of this auxiliary regression.
    \item Under $H_0$, $\mathrm{LM}\sim\chi^2_k$ where $k$ is the number of
          regressors (here $k=1$).
\end{enumerate}

A $p$-value below 0.05 indicates significant heteroscedasticity.

\subsection{Wald--Wolfowitz Runs Test for Randomness}

The runs test \citep{wald1940} assesses whether the sequence of residual signs
(positive/negative) is random.

Let $n_+$ and $n_-$ be the counts of positive and negative residuals, and $R$
the number of runs (maximal consecutive subsequences of the same sign).  Under
$H_0$ (random ordering):
\begin{align}
    \E[R] &= \frac{2n_+n_-}{n_++n_-}+1, \\
    \Var(R) &= \frac{2n_+n_-(2n_+n_--n_+-n_-)}{(n_++n_-)^2(n_++n_--1)}.
\end{align}
The standardised test statistic $Z=(R-\E[R])/\sqrt{\Var(R)}$ is approximately
standard normal for large $n$.

A $p$-value below 0.05 suggests systematic (non-random) structure in the
residuals, indicating potential model misspecification.

% =============================================================================
\section{Heteroscedastic Inverse Prediction}
\label{sec:hetero-inverse}
% =============================================================================

When the user elects to account for heteroscedasticity, the full parameter
vector becomes $(\thetab,\,\bm{\phi})$ where $\bm{\phi}=(\sigma_0,\sigma_1)$
or $\bm{\phi}=(\sigma_0,\delta)$.

The inverse prediction procedure (\Cref{prop:mc-inverse}) is modified as
follows.  For each posterior draw $s$:
\begin{enumerate}[nosep]
    \item Compute the approximate noise level at $y^{*}$:
          \begin{equation}
              \sigma^{*(s)} = g\!\bigl(|y^{*}|;\,\bm{\phi}^{(s)}\bigr),
          \end{equation}
          using $|y^{*}|$ as a proxy for $|\mu_i|$ (since $y^{*}\approx f(x^{*};\thetab)$
          up to noise).
    \item Sample $\tilde{y}^{(s)}=y^{*}+\epsilon^{(s)}$ with
          $\epsilon^{(s)}\sim\N(0,(\sigma^{*(s)})^2)$.
    \item Invert: $x^{*(s)}=f^{-1}(\tilde{y}^{(s)};\,\thetab^{(s)})$.
\end{enumerate}

This correctly propagates the heteroscedastic noise structure into the
inverse-prediction uncertainty.

% =============================================================================
\section{Summary of Assumptions}
\label{sec:assumptions}
% =============================================================================

For clarity we collect all assumptions:

\begin{enumerate}[nosep]
    \item[\ref{ass:functional}] The user-specified functional form $f(x;\thetab)$
          is a correct (or adequate) description of the calibration relationship.
    \item[\ref{ass:noise}] Observation noise is additive and Gaussian with
          constant variance (homoscedastic model) or signal-dependent variance
          (heteroscedastic model, \Cref{ass:hetero}).
    \item[\ref{ass:indep}] Observations are mutually independent conditional on
          the parameters.
    \item[\ref{ass:priors}] Prior distributions are weakly informative
          (zero-centred normals for location parameters, half-normals for scale
          parameters).
    \item[\ref{ass:diff}] The log-posterior is differentiable w.r.t.\ all
          continuous parameters (required for NUTS).
    \item[\ref{ass:new-obs}] New observations follow the same generative process
          as the calibration data (no distribution shift).
\end{enumerate}

\paragraph{When assumptions may be violated.}
\begin{itemize}[nosep]
    \item If the functional form is wrong, residual diagnostics (runs test)
          should flag systematic patterns.
    \item If noise is non-Gaussian (e.g.\ heavy-tailed), the posterior and
          credible intervals may undercover.  Robust likelihood extensions
          (e.g.\ Student-$t$) can be implemented as future work.
    \item If observations are correlated (e.g.\ temporal drift), the
          independence assumption is violated and the credible intervals will be
          too narrow.
\end{itemize}

% =============================================================================
\section{Software Implementation Notes}
\label{sec:software}
% =============================================================================

\begin{itemize}[nosep]
    \item \textbf{Equation parsing:} SymPy's parser with implicit
          multiplication and \texttt{convert\_xor} transformations.
    \item \textbf{Automatic differentiation:} PyTensor (the tensor backend
          of PyMC) provides exact gradients for HMC/NUTS.
    \item \textbf{Sampling:} PyMC v5.10+ with the default NUTS sampler.
    \item \textbf{Diagnostics:} ArviZ for convergence statistics;
          statsmodels for the Breusch--Pagan and runs tests.
    \item \textbf{Numerical inversion:} SciPy's \texttt{brentq} with
          tolerance $10^{-10}$.
\end{itemize}

% =============================================================================
% References
% =============================================================================
\bibliographystyle{plainnat}

\begin{thebibliography}{9}

\bibitem[Breusch and Pagan(1979)]{breusch1979}
T.~S. Breusch and A.~R. Pagan.
\newblock A simple test for heteroscedasticity and random coefficient variation.
\newblock \emph{Econometrica}, 47(5):1287--1294, 1979.

\bibitem[Hoffman and Gelman(2014)]{hoffman2014}
M.~D. Hoffman and A.~Gelman.
\newblock The {No-U-Turn} sampler: adaptively setting path lengths in
  {H}amiltonian {M}onte {C}arlo.
\newblock \emph{Journal of Machine Learning Research}, 15:1593--1623, 2014.

\bibitem[Nesterov(2009)]{nesterov2009}
Y.~Nesterov.
\newblock Primal-dual subgradient methods for convex problems.
\newblock \emph{Mathematical Programming}, 120(1):221--259, 2009.

\bibitem[Wald and Wolfowitz(1940)]{wald1940}
A.~Wald and J.~Wolfowitz.
\newblock On a test whether two samples are from the same population.
\newblock \emph{The Annals of Mathematical Statistics}, 11(2):147--162, 1940.

\bibitem[Gelman et~al.(2013)]{gelman2013}
A.~Gelman, J.~B. Carlin, H.~S. Stern, D.~B. Dunson, A.~Vehtari, and
  D.~B. Rubin.
\newblock \emph{Bayesian Data Analysis}.
\newblock Chapman \& Hall/CRC, 3rd edition, 2013.

\bibitem[Salvatier et~al.(2016)]{salvatier2016}
J.~Salvatier, T.~V. Wiecki, and C.~Fonnesbeck.
\newblock Probabilistic programming in {P}ython using {PyMC3}.
\newblock \emph{PeerJ Computer Science}, 2:e55, 2016.

\bibitem[Neal(2011)]{neal2011}
R.~M. Neal.
\newblock {MCMC} using {H}amiltonian dynamics.
\newblock In S.~Brooks, A.~Gelman, G.~L. Jones, and X.-L. Meng, editors,
  \emph{Handbook of Markov Chain Monte Carlo}, chapter~5. Chapman \&
  Hall/CRC, 2011.

\bibitem[Eisenhart(1939)]{eisenhart1939}
C.~Eisenhart.
\newblock The interpretation of certain regression methods and their use in
  biological and industrial research.
\newblock \emph{The Annals of Mathematical Statistics}, 10(2):162--186, 1939.

\bibitem[Hoadley(1970)]{hoadley1970}
B.~Hoadley.
\newblock A {B}ayesian look at inverse linear regression.
\newblock \emph{Journal of the American Statistical Association},
  65(329):356--369, 1970.

\end{thebibliography}

% =============================================================================
\end{document}
% =============================================================================