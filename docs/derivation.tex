% ============================================================================
% Bayesian Calibration & Inverse Prediction â€” Mathematical Derivation
% ============================================================================
% Compile:  pdflatex derivation.tex   (run twice for references)
% ============================================================================
\documentclass[11pt,a4paper]{article}

% ---- packages --------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{bm}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{fancyhdr}

\geometry{margin=2.5cm}
\hypersetup{colorlinks=true,linkcolor=blue!70!black,citecolor=green!50!black,urlcolor=blue!60!black}

% ---- theorem environments ---------------------------------------------------
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}

% ---- shortcuts --------------------------------------------------------------
\newcommand{\xcal}{\bm{x}^{\mathrm{cal}}}
\newcommand{\ycal}{\bm{y}^{\mathrm{cal}}}
\newcommand{\thetab}{\bm{\theta}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\HN}{\mathcal{N}^{+}}

% ---- header / footer --------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\rhead{\small Bayesian Calibration --- Mathematical Derivation}
\lhead{\small McClelland Lab, UCL}
\cfoot{\thepage}

% =============================================================================
\begin{document}
% =============================================================================

\begin{center}
    {\LARGE\bfseries Bayesian Calibration \&\\[4pt] Inverse Prediction}\\[14pt]
    {\large Mathematical Derivation and Assumptions}\\[10pt]
    {\normalsize McClelland Lab, University College London}\\[4pt]
    {\small\today}
\end{center}

\vspace{1em}
\hrule
\vspace{0.5em}
\tableofcontents
\vspace{0.5em}
\hrule
\vspace{1.5em}

% =============================================================================
\section{Introduction}
% =============================================================================

In analytical chemistry and bioassay work a common workflow is to prepare
standards at known concentrations, measure the instrument response for each,
and fit a calibration curve.  The practical goal, however, is the
\emph{reverse}: given a new instrument reading, estimate the unknown
concentration that produced it.

This is the \textbf{inverse prediction} problem.  Classical approaches
(Fieller's theorem, Wald intervals) provide approximate confidence intervals
but rely on asymptotic normality, struggle with nonlinear models, and do not
fully propagate parameter uncertainty.  A Bayesian treatment resolves these
limitations naturally: we obtain the full joint posterior of the model
parameters, then push every source of uncertainty---parameter estimation
\emph{and} measurement noise---through the inverse to get a complete
distribution over the unknown concentration.

This document derives the method implemented in the accompanying Streamlit
application.

% =============================================================================
\section{Notation}
% =============================================================================

\begin{center}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{cl}
    \toprule
    Symbol & Meaning \\
    \midrule
    $n$ & Number of calibration points \\
    $x_i$ & Known standard value (e.g.\ concentration) for point $i$ \\
    $y_i$ & Measured instrument response for point $i$ \\
    $\xcal = (x_1,\dots,x_n)^\top$ & Calibration $x$-values \\
    $\ycal = (y_1,\dots,y_n)^\top$ & Calibration $y$-values \\
    $f(x;\,\thetab)$ & User-specified calibration function \\
    $\thetab = (\theta_1,\dots,\theta_p)^\top$ & Model parameters \\
    $\sigma$ & Noise standard deviation \\
    $y^{*}$ & New observed response \\
    $x^{*}$ & Unknown value to be estimated \\
    \bottomrule
\end{tabular}
\end{center}

% =============================================================================
\section{The Calibration Model}
\label{sec:model}
% =============================================================================

We begin with three assumptions that define the generative model for the
calibration data.

\begin{assumption}[Calibration function]\label{ass:func}
The relationship between the known value $x$ and the instrument response $y$ is
described by a parametric function $f:\R\times\R^p\to\R$, specified by the
user.  For example, $f(x;\,a,b)=a+bx$, or $f(x;\,a,b)=a\,e^{bx}$.  We require
$f$ to be continuous and differentiable almost everywhere with respect to both
$x$ and~$\thetab$.
\end{assumption}

\begin{assumption}[Gaussian noise]\label{ass:noise}
Each observation is the true calibration value plus independent, identically
distributed Gaussian noise:
\begin{equation}\label{eq:obs-model}
    y_i \;=\; f(x_i;\,\thetab) \;+\; \varepsilon_i,
    \qquad
    \varepsilon_i \;\stackrel{\mathrm{iid}}{\sim}\; \N(0,\,\sigma^2),
    \qquad i = 1,\dots,n.
\end{equation}
The noise terms are mutually independent and independent of the standard
values~$x_i$.
\end{assumption}

\begin{assumption}[Differentiability]\label{ass:diff}
The log-posterior is differentiable with respect to all continuous parameters.
This is guaranteed by the SymPy-parsed equation and PyTensor's automatic
differentiation, and is required by the gradient-based NUTS sampler
(\Cref{sec:nuts}).
\end{assumption}

\noindent
Under these assumptions the \textbf{likelihood} is
\begin{equation}\label{eq:likelihood}
    p(\ycal \mid \xcal,\,\thetab,\,\sigma)
    \;=\; \prod_{i=1}^{n}
        \frac{1}{\sqrt{2\pi}\,\sigma}\,
        \exp\!\biggl[-\frac{\bigl(y_i - f(x_i;\,\thetab)\bigr)^{2}}
                           {2\,\sigma^{2}}\biggr],
\end{equation}
or equivalently, writing
$\bm{\mu} = \bigl(f(x_1;\thetab),\dots,f(x_n;\thetab)\bigr)^\top$,
\begin{equation}
    \ycal \mid \thetab,\,\sigma
    \;\sim\;
    \N\!\bigl(\bm{\mu},\;\sigma^{2}\,\mathbf{I}_{n}\bigr).
\end{equation}

% =============================================================================
\section{Prior Distributions}
\label{sec:priors}
% =============================================================================

We place weakly informative, independent priors on all parameters:
\begin{align}
    \theta_j &\sim \N(0,\,10^{2}),
        &&j = 1,\dots,p, \label{eq:prior-theta} \\[4pt]
    \sigma &\sim \HN(10),
        \label{eq:prior-sigma}
\end{align}
where $\HN(\tau)$ denotes the half-normal distribution with scale~$\tau$,
i.e.\ a $\N(0,\tau^{2})$ truncated to the positive reals.

\paragraph{Rationale.}
The zero-centred normal priors on each~$\theta_j$ are deliberately vague
(standard deviation~10).  The half-normal prior on~$\sigma$ enforces positivity
while remaining uninformative over the plausible range of noise magnitudes.
These defaults perform well across a broad class of calibration problems.  Users
with strong domain knowledge may substitute tighter priors in the source code.

\paragraph{Joint prior.}
Because the priors are independent,
\begin{equation}\label{eq:joint-prior}
    p(\thetab,\,\sigma)
    \;=\;
    \biggl[\prod_{j=1}^{p} p(\theta_j)\biggr]\;p(\sigma).
\end{equation}

% =============================================================================
\section{Posterior Distribution}
\label{sec:posterior}
% =============================================================================

Applying Bayes' theorem:
\begin{equation}\label{eq:bayes}
    \boxed{\;
    p(\thetab,\,\sigma \mid \ycal,\,\xcal)
    \;=\;
    \frac{p(\ycal \mid \xcal,\,\thetab,\,\sigma)\;\;
          p(\thetab,\,\sigma)}
         {p(\ycal \mid \xcal)}
    \;}
\end{equation}
where the marginal likelihood (evidence) is
\begin{equation}
    p(\ycal \mid \xcal)
    \;=\;
    \int p(\ycal \mid \xcal,\,\thetab,\,\sigma)\;
         p(\thetab,\,\sigma)
    \;\mathrm{d}\thetab\;\mathrm{d}\sigma.
\end{equation}
For most nonlinear calibration functions this integral is analytically
intractable, so we approximate the posterior using Markov chain Monte Carlo
sampling.

% =============================================================================
\section{MCMC Sampling via NUTS}
\label{sec:nuts}
% =============================================================================

\subsection{Hamiltonian Monte Carlo}

Hamiltonian Monte Carlo (HMC) \citep{neal2011} augments the parameter space
with auxiliary momentum variables $\bm{r}\in\R^{d}$ (where $d = p+1$) and
defines the joint density
\begin{equation}
    p(\thetab,\,\sigma,\,\bm{r})
    \;\propto\;
    \exp\!\bigl[\,-U(\thetab,\sigma)
                 - \tfrac{1}{2}\,\bm{r}^{\top}\mathbf{M}^{-1}\bm{r}\,\bigr],
\end{equation}
with \emph{potential energy}
\begin{equation}
    U(\thetab,\sigma)
    \;=\;
    -\log p(\ycal \mid \xcal,\,\thetab,\,\sigma)
    \;-\; \log p(\thetab,\,\sigma)
\end{equation}
and mass matrix $\mathbf{M}$ (adapted during warm-up to approximate the
posterior covariance).

HMC simulates Hamiltonian dynamics using the leapfrog integrator with step
size~$\epsilon$:
\begin{align}
    \bm{r}_{t+\epsilon/2}
        &= \bm{r}_{t}
           - \tfrac{\epsilon}{2}\,\nabla_{\thetab}\,U(\thetab_{t}),
    \\
    \thetab_{t+\epsilon}
        &= \thetab_{t}
           + \epsilon\,\mathbf{M}^{-1}\,\bm{r}_{t+\epsilon/2},
    \\
    \bm{r}_{t+\epsilon}
        &= \bm{r}_{t+\epsilon/2}
           - \tfrac{\epsilon}{2}\,\nabla_{\thetab}\,U(\thetab_{t+\epsilon}).
\end{align}
After $L$ leapfrog steps the proposal is accepted with probability
$\min\!\bigl(1,\,\exp(-\Delta H)\bigr)$, where $\Delta H$ is the change in
the Hamiltonian.

\subsection{The No-U-Turn Sampler}

NUTS \citep{hoffman2014} removes the need to choose~$L$ by building a balanced
binary tree of leapfrog steps.  The tree doubles in size until a ``U-turn''
is detected:
\begin{equation}
    \bm{r}\cdot(\thetab^{+}-\thetab^{-}) < 0
    \qquad\text{or}\qquad
    \bm{r}\cdot(\thetab^{-}-\thetab^{+}) < 0,
\end{equation}
where $\thetab^{+}$ and $\thetab^{-}$ are the trajectory endpoints.
A multinomial scheme selects the next state from the trajectory, weighted by
the unnormalised density.  The step size~$\epsilon$ is tuned during warm-up
via dual averaging \citep{nesterov2009} to target an acceptance rate of
${\sim}\,0.8$.

\subsection{Warm-up, Sampling, and Diagnostics}

\begin{enumerate}[nosep]
    \item \textbf{Warm-up.}  The step size and mass matrix are adapted; these
          draws are discarded.
    \item \textbf{Sampling.}  $S$~draws are collected from each of
          $C$~independent chains, giving $N = S \times C$ posterior samples
          $\{(\thetab^{(s)},\,\sigma^{(s)})\}_{s=1}^{N}$.
    \item \textbf{Convergence checks.}  The application reports:
    \begin{itemize}[nosep]
        \item $\hat{R}$ (Gelman--Rubin statistic): values $\lesssim 1.01$
              indicate convergence \citep{gelman2013}.
        \item Effective sample size ($n_{\mathrm{eff}}$): the number of
              effectively independent draws after accounting for
              autocorrelation.
        \item Monte Carlo standard error (MCSE): the precision of the
              posterior mean estimate.
    \end{itemize}
\end{enumerate}

% =============================================================================
\section{Inverse Prediction}
\label{sec:inverse}
% =============================================================================

\subsection{Problem Statement}

Given a new instrument reading $y^{*}$, we want the posterior predictive
distribution of the unknown value $x^{*}$ that produced it.  We assume the new
observation arises from the same process as the calibration data:
\begin{equation}\label{eq:new-obs}
    y^{*} \;=\; f(x^{*};\,\thetab) \;+\; \varepsilon^{*},
    \qquad
    \varepsilon^{*} \sim \N(0,\,\sigma^{2}).
\end{equation}

\subsection{Posterior Predictive Distribution of \texorpdfstring{$x^{*}$}{x*}}

The target distribution is obtained by marginalising over the posterior:
\begin{equation}\label{eq:ppd}
    \boxed{\;
    p(x^{*} \mid y^{*},\,\ycal,\,\xcal)
    \;=\;
    \int p(x^{*} \mid y^{*},\,\thetab,\,\sigma)\;\;
         p(\thetab,\,\sigma \mid \ycal,\,\xcal)
    \;\mathrm{d}\thetab\;\mathrm{d}\sigma
    \;}
\end{equation}
This integral propagates \emph{both} parameter uncertainty and measurement
noise into the prediction.  We evaluate it by Monte Carlo:

\begin{proposition}[Monte Carlo inverse prediction]\label{prop:mc}
For each posterior draw $(\thetab^{(s)},\,\sigma^{(s)})$, $s=1,\dots,N$:
\begin{enumerate}[nosep]
    \item \textbf{Add noise:}\;
          $\tilde{y}^{(s)} = y^{*} + \epsilon^{(s)}$\;
          where\; $\epsilon^{(s)} \sim \N(0,\,\sigma^{(s)2})$.
    \item \textbf{Invert:}\;
          $x^{*(s)} = f^{-1}\!\bigl(\tilde{y}^{(s)};\,\thetab^{(s)}\bigr)$.
\end{enumerate}
The resulting collection $\{x^{*(s)}\}_{s=1}^{N}$ is a sample from
$p(x^{*} \mid y^{*},\,\ycal,\,\xcal)$.
\end{proposition}

\begin{proof}
For fixed $(\thetab,\sigma)$ the forward model is deterministic, so
$x^{*} = f^{-1}(y^{*} - \varepsilon^{*};\,\thetab)$ with
$\varepsilon^{*}\sim\N(0,\sigma^{2})$.  Drawing
$\tilde{y}^{(s)} = y^{*} + \epsilon^{(s)}$ where
$\epsilon^{(s)}\sim\N(0,\sigma^{(s)2})$ is equivalent to sampling the
noise-free response from the predictive distribution at draw~$s$.
Since the draws $(\thetab^{(s)},\sigma^{(s)})$ come from the posterior
$p(\thetab,\sigma\mid\ycal,\xcal)$, the composition produces samples from the
marginal~\eqref{eq:ppd}.
\end{proof}

\subsection{Inversion Methods}

\paragraph{Symbolic inverse.}
When $f$ is algebraically invertible, SymPy computes
$x = f^{-1}(y;\,\thetab)$ in closed form.  If multiple real solutions exist
(e.g.\ for a quadratic), the one closest to the centroid of the calibration
$x$-values is selected.

\paragraph{Numerical inverse.}
When no closed form exists, Brent's root-finding method
\citep{brent1973} solves
\begin{equation}
    f(x;\,\thetab^{(s)}) - \tilde{y}^{(s)} = 0
\end{equation}
over the interval
$\bigl[x_{\min} - 3\,\Delta x,\;\; x_{\max} + 3\,\Delta x\bigr]$
where $\Delta x = x_{\max} - x_{\min}$.
Brent's method combines bisection, secant, and inverse quadratic
interpolation, guaranteeing convergence whenever a sign change exists.

\subsection{Credible Intervals and Point Estimates}

From the $N$ draws $\{x^{*(s)}\}$ (after discarding any non-finite values from
failed inversions), the $100(1-\alpha)\%$ equal-tailed credible interval is
\begin{equation}
    \mathrm{CI}_{1-\alpha}
    \;=\;
    \bigl[\,Q_{\alpha/2},\;\; Q_{1-\alpha/2}\,\bigr],
\end{equation}
where $Q_{q}$ is the $q$-th sample quantile.  The application also reports:
\begin{align}
    \hat{x}_{\mathrm{median}} &= Q_{0.5}, \\[2pt]
    \hat{x}_{\mathrm{mean}}
        &= \frac{1}{N}\sum_{s=1}^{N} x^{*(s)}, \\[2pt]
    \hat{\sigma}_{x}
        &= \sqrt{\frac{1}{N-1}
           \sum_{s=1}^{N}\bigl(x^{*(s)} - \hat{x}_{\mathrm{mean}}\bigr)^{2}}.
\end{align}

% =============================================================================
\section{Residual Diagnostics}
\label{sec:diagnostics}
% =============================================================================

After fitting the model, the application runs automated diagnostics to help the
user assess whether the chosen equation and the noise assumptions are adequate.

\subsection{Residuals}

Residuals are evaluated at the posterior median parameters
$\hat{\thetab} = \mathrm{median}\{\thetab^{(s)}\}$:
\begin{equation}
    e_i \;=\; y_i \;-\; f(x_i;\,\hat{\thetab}),
    \qquad i = 1,\dots,n.
\end{equation}
Two plots are shown: residuals versus fitted values~$\hat{y}_i$ and residuals
versus~$x_i$.

\subsection{Breusch--Pagan Test}

The Breusch--Pagan test \citep{breusch1979} checks whether the variance of
the residuals depends on the fitted values:
\begin{equation}
    H_0:\;\Var(\varepsilon_i) = \sigma^{2}\;\;\forall\,i
    \qquad\text{vs.}\qquad
    H_1:\;\Var(\varepsilon_i) = h(\bm{z}_i^{\top}\bm{\gamma}).
\end{equation}
The procedure regresses the squared residuals $e_i^{2}$ on the fitted
values by OLS and computes the LM statistic
$\mathrm{LM} = \tfrac{1}{2}\,\mathrm{ESS}$, which follows a $\chi^{2}_{1}$
distribution under $H_0$.  A $p$-value below 0.05 indicates that the noise
spread changes with signal level.  In that case, the application advises
applying a variance-stabilising transformation (e.g.\
$\log y$ or $\sqrt{y}$) before re-fitting.

\subsection{Wald--Wolfowitz Runs Test}

The runs test \citep{wald1940} assesses whether the sign pattern of the
residuals ($+$/$-$) is random.
Let $n_{+}$ and $n_{-}$ be the counts of positive and negative residuals, and
$R$ the number of runs.  Under $H_0$ (random ordering):
\begin{align}
    \E[R]
        &= 1 + \frac{2\,n_{+}\,n_{-}}{n_{+}+n_{-}}\,,
    \\[4pt]
    \Var(R)
        &= \frac{2\,n_{+}\,n_{-}\,(2\,n_{+}\,n_{-} - n_{+} - n_{-})}
               {(n_{+}+n_{-})^{2}\,(n_{+}+n_{-}-1)}\,.
\end{align}
The standardised statistic $Z = (R - \E[R])/\!\sqrt{\Var(R)}$ is approximately
standard normal.  A $p$-value below 0.05 suggests systematic structure that the
model does not capture; the user should consider a different functional form.

\subsection{Practical Guidance}

Based on the diagnostics the application presents the following advice:
\begin{itemize}[nosep]
    \item \textbf{Curved / trending residuals:} try a different model (e.g.\
          add a polynomial term, switch to an exponential or power law).
    \item \textbf{Fan-shaped spread:} try a variance-stabilising
          transformation such as $\log(y)$ or $\sqrt{y}$ before fitting.
    \item \textbf{Isolated outliers:} check for data-entry errors; consider
          excluding the suspect point and re-fitting.
    \item \textbf{Random scatter around zero:} the model assumptions appear
          satisfied.
\end{itemize}

% =============================================================================
\section{Summary of Assumptions}
\label{sec:assumptions}
% =============================================================================

\begin{enumerate}[nosep,leftmargin=2em]
    \item The user-specified function $f(x;\thetab)$ adequately describes the
          calibration relationship
          (\Cref{ass:func}).
    \item Observation noise is additive, Gaussian, and i.i.d.\ with constant
          variance~$\sigma^{2}$
          (\Cref{ass:noise}).
    \item The log-posterior is differentiable with respect to all continuous
          parameters
          (\Cref{ass:diff}).
    \item Prior distributions are weakly informative and mutually independent
          (\Cref{sec:priors}).
    \item New measurements follow the same generative process as the
          calibration data (no distribution shift).
\end{enumerate}

\paragraph{When assumptions may be violated.}
\begin{itemize}[nosep]
    \item \emph{Wrong functional form.}  The runs test will flag non-random
          residual patterns; try a different equation.
    \item \emph{Non-constant variance.}  The Breusch--Pagan test will flag
          this; apply a variance-stabilising transform (e.g.\ $\log y$) and
          re-fit.
    \item \emph{Non-Gaussian noise.}  Heavy tails may cause credible intervals
          to undercover.  A Student-$t$ likelihood could be added in future.
    \item \emph{Correlated observations.}  If measurements are time-dependent,
          the independence assumption is violated and intervals will be too
          narrow.
\end{itemize}

% =============================================================================
\section{Implementation Notes}
\label{sec:software}
% =============================================================================

\begin{itemize}[nosep]
    \item \textbf{Equation parsing:} SymPy with implicit multiplication and
          \texttt{convert\_xor} transformations.
    \item \textbf{Automatic differentiation:} PyTensor provides exact
          gradients for the NUTS sampler.
    \item \textbf{Sampling:} PyMC\,$\ge$\,5.10 with the default NUTS
          implementation.
    \item \textbf{Diagnostics:} ArviZ for convergence statistics; statsmodels
          for the Breusch--Pagan test; the Wald--Wolfowitz test is implemented
          directly using SciPy.
    \item \textbf{Numerical inversion:} SciPy's \texttt{brentq} with
          tolerance~$10^{-10}$.
\end{itemize}

% =============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Breusch and Pagan(1979)]{breusch1979}
T.~S. Breusch and A.~R. Pagan.
\newblock A simple test for heteroscedasticity and random coefficient variation.
\newblock \emph{Econometrica}, 47(5):1287--1294, 1979.

\bibitem[Brent(1973)]{brent1973}
R.~P. Brent.
\newblock \emph{Algorithms for Minimization without Derivatives}.
\newblock Prentice-Hall, 1973.

\bibitem[Gelman et~al.(2013)]{gelman2013}
A.~Gelman, J.~B. Carlin, H.~S. Stern, D.~B. Dunson, A.~Vehtari, and
D.~B. Rubin.
\newblock \emph{Bayesian Data Analysis}.
\newblock Chapman \& Hall/CRC, 3rd edition, 2013.

\bibitem[Hoffman and Gelman(2014)]{hoffman2014}
M.~D. Hoffman and A.~Gelman.
\newblock The {No-U-Turn} sampler: adaptively setting path lengths in
{Hamiltonian Monte Carlo}.
\newblock \emph{Journal of Machine Learning Research}, 15:1593--1623, 2014.

\bibitem[Neal(2011)]{neal2011}
R.~M. Neal.
\newblock {MCMC} using {Hamiltonian} dynamics.
\newblock In S.~Brooks, A.~Gelman, G.~L. Jones, and X.-L. Meng, editors,
\emph{Handbook of Markov Chain Monte Carlo}, chapter~5.
Chapman \& Hall/CRC, 2011.

\bibitem[Nesterov(2009)]{nesterov2009}
Y.~Nesterov.
\newblock Primal-dual subgradient methods for convex problems.
\newblock \emph{Mathematical Programming}, 120(1):221--259, 2009.

\bibitem[Salvatier et~al.(2016)]{salvatier2016}
J.~Salvatier, T.~V. Wiecki, and C.~Fonnesbeck.
\newblock Probabilistic programming in {Python} using {PyMC3}.
\newblock \emph{PeerJ Computer Science}, 2:e55, 2016.

\bibitem[Wald and Wolfowitz(1940)]{wald1940}
A.~Wald and J.~Wolfowitz.
\newblock On a test whether two samples are from the same population.
\newblock \emph{The Annals of Mathematical Statistics}, 11(2):147--162, 1940.

\end{thebibliography}

% =============================================================================
\end{document}
% =============================================================================